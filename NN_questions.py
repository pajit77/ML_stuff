# -*- coding: utf-8 -*-
"""
Created on Sat Oct 21 19:14:27 2017

@author: Pankaj
"""

#We expect one file for this section:
#NN_questions.{py|java|c|cpp}
#that prints out the answers to the questions below. Each question is worth 3 points.
#1. Stopping criteria: One of the usual practices to decide when to stop the training is by finding
#the point where the error starts to increase in the: a) training set b) development set
print 'b'

#2. If we use a large learning rate, we may face a problem of: a) overshooting b) overfitting
print 'a'

#3. A single artificial neuron with linear/identity transfer function can represent the functions that
#a linear regression can represent: yes/no
print 'yes'
#4. A single artificial neuron with sigmoid transfer function can represent the functions that a logistic
#regression can represent: yes/no
print 'yes'

#5. Stochastic gradient descent helps faster convergence compared to the (full) gradient descent:
#yes/no
print 'yes'

#6. Stochastic gradient descent guarantees strictly decreasing error every update: yes/no
print 'no'

#7. Normalizing the input data may help faster convergence: yes/no
print 'yes'

#8. In practice, the weights are usually initialized to: a) zeros b) random small numbers c) random
#numbers in the range [-100,+100]
print 'b'

#9. How many connections (weights) are there between two hidden layers that both have 5 neurons
#in each layer? (Ignore bias neurons) : a) 15 b) 25 c) 50
print 'b'

#10. In backpropagation, the usual practice is that we start with small learning rate and increase it
#as we train: yes/no
print 'no'


